<!DOCTYPE html>
<html>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<style>

html{
 
}

body {

}

.fa {
  padding: 10px;
  font-size: 15px;
  width: 15px;
  height: 15px;
  text-align: center;
  text-decoration: none;
  margin: 5px 2px;
  border-radius: 50%;
}

.fa:hover {
    opacity: 0.7;
}

.fa-twitter {
  background: #55ACEE;
  color: white;
}

.fa-linkedin {
  background: #007bb5;
  color: white;
}

.fa-skype {
  background: #00aff0;
  color: white;
}

.fa-telegram {
  background: #00aff0;
  color: white;
}

.fa-github {
  background: #00aff0;
  color: white;
}

img {
  float: left;
  width: 20%;
  height: auto;
  margin-right:15px;
  border-radius: 50%;
}

/* ---- NEW: simple tab styles ---- */
.tab-buttons {
  margin-top: 20px;
  margin-bottom: 10px;
  border-bottom: 1px solid #ddd;  
}

.tab-buttons button {
  background-color: white;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 10px 16px;
  font-size: 16px;
  color: #6b5b95;
}

.tab-buttons button.active {
  border-bottom: 3px solid #ff7b25;
  font-weight: bold;
}

.tab-section {
  display: none;
}

.tab-section.active {
  display: block;
}
ul li {
  margin-bottom: 8px;   /* space between items */
}

#projects {
  padding-top: 1px;   /* or padding-bottom */
}
.projects-grid {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 24px;          /* space between the three divs */
}
.project-card {
  display: flex;
  align-items: flex-start;
  margin-bottom: 16px;
}

.project-image {
  width: 120px;
  height: auto;
  border-radius: 8px;
  margin-right: 16px;
}

.project-content {
  max-width: 600px;
}

.project-title {
  margin: 0 0 4px 0;
  font-size: 18px;
  color: #ff7b25;
}

.project-links a {
  color: #6b5b95;
  text-decoration: none;
  margin-right: 8px;
}

.project-links a:hover {
  text-decoration: underline;
}

.project-text {
  margin-top: 6px;
}
.project-text,
.abstract-text {
  text-align: justify;
}
footer{
 position: fixed;
 bottom: 25px;
}
</style>

<head>
<title>Alesia Chernikova</title>
</head>

<body>

<div class="bg"></div>
<div style="margin-left: 10%; margin-right: 10%; display:block; align:center">

  <!-- Header / profile block stays the same -->
  <p><img src="aradlab.jpg" alt="Avatar" width="160" height="200"></p>
  <p style="font-size: 35px;color: #d64161; font-weight: bold">Alesia Chernikova</p>
  <p style="font-size: 20px;color: #feb236">
    Postdoctoral Research Associate</br>
    a.chernikova@northeastern.edu</br>
    <a href="Industry_Chernikova_CV.pdf"
       style="color:  #feb236; text-decoration: none">CV</a>
  </p>

  <!-- NEW: tab buttons -->
  <div class="tab-buttons">
    <button class="tab-link active" data-target="about">About</button>
   
    <button class="tab-link" data-target="publications">Publications</button>
    <button class="tab-link" data-target="teaching">Teaching</button>
    <button class="tab-link" data-target="talks">Talks</button> 
    <button class="tab-link" data-target="service">Service</button> 

  </div>

  <!-- NEW: ABOUT tab (wrap your original intro text) -->
  <div id="about" class="tab-section active" style="font-size: 17px">
    <p>
      Hi, I am a Postdoctoral Research Associate at the Network Science Institute at Northeastern University.
      I work under the supervision of Professor
      <a href='http://eliassi.org' style="color: #6b5b95; text-decoration: none">Tina Eliassi-Rad</a>
      and in collaboration with Professor
      <a href='https://www.dk-lab.net/personnel.html' style="color: #6b5b95; text-decoration: none">Dmitri Krioukov</a>.
      <br/><br/>
      My research develops foundation that explains how AI models operate and principled methods that ensure their reliability as capabilities continue to accelerate.
      Currently I am working on the <font style="color: #ff7b25">mechanistic interpretability of Large Language Models</font> to assess whether their internal computations are predictable, reliable, and aligned with human intent. 
      I also developed a unified framework for <font style="color: #ff7b25">uncertainty quantification, certified adversarial robustness, and generalization in Message-Passing Neural Networks </font>  to reason about how they operate under the uncertainty in node features.
      Anoter direction of my research focuses on <font style="color: #ff7b25"> grounding AI architecture design in the geometric principles</font> and hierarchical organization of natural neural networks and offers a principled path toward understanding how the resulting structural properties are related to learning efficiency and generalization.
      <br/><br/>
      I completed my Ph.D. in Computer Science at Northeastern University, where I was advised by Professor
      <a href='https://www.khoury.northeastern.edu/home/alina/'
         style="color: #6b5b95; text-decoration: none">Alina Oprea</a>.
      I was affiliated with the
      <a href="https://nds2.ccs.neu.edu" style="color: #6b5b95; text-decoration: none">NDS2 lab</a>
      and was a part of the Cybersecurity and Privacy Institute. My researcg focused on <font style="color: #ff7b25"> adversarial machine learning and cybernetwork resilience</font>. I developed optimization-based frameworks for feasible evasion attacks under real-world constraints, with applications to cybersecurity. This work informed deployments at the Army Research Laboratory and Perspecta Labs. Our collaboration with Toyota Motor North America R&D Institute resulted in the first demonstrated evasion attacks against deep neural networks used for autonomous vehicle steering prediction. In parallel, I introduced a novel epidemiological model for malware propagation and graph-based defense strategies evaluated on enterprise-scale communication networks.
      <br/><br/>
      I spent two summers working as Applied Research Scientist at Amazon Web Services in Amazon Detective team. I created a <font style="color: #ff7b25"> scalable algorithm for tracing the activity in the AWS cloud </font> represented as a heterogeneous graph to allow further research based on AWS cloud activity data. I also developed the methodology for <font style="color: #ff7b25"> lateral movement detection in the AWS cloud</font> environment using Bayesian statistics and network science perspectives.
      <br/><br/>    
      I received my BS degree in Applied Mathematics and Computer Science from
      Belarusian State University, where I was affiliated with the Mathematical Modeling and Data Analysis Department
      under the supervision of Professor
      <a href='https://scholar.google.ru/citations?user=bT04cCIAAAAJ&hl=ru'
         style="color: #6b5b95; text-decoration: none">Vladimir Malugin</a>.
      My research focus included the design of  <font style="color: #ff7b25">hedging algorithms based on derivative contracts</font>. Additionally, I was a
      part of the Research Institute of Applied Mathematics and Information Technology Problems, where I participated
      in the project for  <font style="color: #ff7b25">credit rankings estimation</font> and evaluation of national enterprises using mathematical,
      statistical, and econometric methods and models. <br/><br/>
      After my undegraduate studies, I worked as a senior software engineer at IBA Group in Minsk, Belarus. I participated in the development of a <font style="color: #ff7b25">large-scale IBM GSAR web portal</font> and maintained its performance.
    </p>

    <p>
      My other interests include hiking and yoga,
      <a href='https://www.instagram.com/chromatic_film_photos/'
         style="color: #6b5b95; text-decoration: none">film photography</a>
      and visual art, music and soundscapes.
    </p>
  </div>

  <!-- NEW: PROJECTS tab (placeholder content you can edit) -->
  <div id="projects" class="tab-section" style="font-size: 17px">
      <div class="projects-grid">
      <div class="project-card">
           
    <div class="project-content">
      <h4 class="project-title">Automatic Circuit Extraction in Cross-Layer Transcoders</h4>

      <p class="project-text">
      As modern AI systems, including LLMs, grow in scale and complexity, mechanistic interpretability has become essential to assess whether their internal computations are predictable, reliable, and aligned with human intent. Recent work has shown that sparse coding–based techniques, including sparse autoencoders, transcoders, and crosscoders, can isolate interpretable features in model activations, with many such features corresponding to meaningful semantic concepts. Anthropic’s introduction of Cross-Layer Transcoders (CLTs) and the Circuit Tracer tool represents a significant step forward: CLTs construct attribution graphs that trace how information propagates through an LLM for a given input, revealing groups of interacting neurons that plausibly form computational circuits. However, translating these dense attribution graphs into a concise, mechanistic explanation still relies heavily on human inspection.

      My research develops a fully automated framework to extract such circuits from CLT-generated attribution graphs. The approach begins with pruning based on importance weights, removing weak or irrelevant edges while preserving the dominant computational backbone. On this reduced graph, I apply a flow-based extraction method grounded in an information-flow analogy to electrical networks, which identifies pathways between selected source and target neurons. The resulting subgraph provides a candidate circuit that can be systematically interpreted by mapping its components to linguistic or semantic functions. This framework transforms a previously manual, ad hoc process into a scalable, principled methodology for uncovering the computational structure within LLMs, offering a path toward more transparent and trustworthy AI systems.

      </p>
    </div>
      </div>
       <div class="project-card">
      <div class="project-content">
      <h4 class="project-title">Robustness and Generalization in Uncertainty-aware Message Passing Neural Networks</h4>

      <p class="project-text">
     Existing theoretical guarantees for message passing neural networks (MPNNs) assume deterministic node features. We address the more realistic situation where noise or finite measurement precision lead to uncertainties in the values of node features. First, we quantify uncertainty by propagating the moments of node-feature distributions through the MPNN architecture. To propagate the moments through activation functions, we use second-order Taylor expansion and pseudo-Taylor polynomial expansion (PTPE). We use the resulting node embedding distributions to analytically generate probabilistic adversarial robustness certificates for node classification tasks against the L2-bounded perturbations of the node features. Second, we model node features as multivariate random variables and propose a Wasserstein distance-based pseudo-metric, the Feature Convolution Distance FCDp, corresponding to the discriminative power of MPNNs at the node level. We show that MPNNs are global Lipschitz continuous functions with respect to the introduced pseudo-metric FCDp. Using the covering number of the resulting pseudometric space (which is a subset of the Wasserstein space), we derive generalization bounds for MPNNs with uncertainties in the node features. Together, these two complementary approaches --- moment propagation for probabilistic robustness and the FCDp on the subset of the Wasserstein space for generalization --- 
establish a unified theoretical framework that comprehensively addresses MPNN reliability under node feature uncertainty.
      </p>
    </div>
       </div>
<div class="project-card">
    <div class="project-content">
      <h4 class="project-title">Uncertainty-aware Message Passing Neural Networks</h4>
      

      <p class="project-links">
        <a href="https://openreview.net/pdf?id=bVToNxLO9W" target="_blank"> NeurIPS 2025: NPGML</a>
      </p>

      <p class="project-text">
      Existing theoretical guarantees for message passing neural networks (MPNNs) assume deterministic node features, whereas in this work we address the more realistic setting where inherent noise or finite measurement precision leads to uncertainty in node features. We assume node features are multivariate Gaussian distributions and propagate
      their first and second moments through the MPNN architecture. We employ Polynomial Chaos Expansion to approximate nonlinearities,
      and use the resulting node embedding distributions to analytically produce probabilistic node-wise robustness certificates against 
      L2-bounded node feature perturbations. Moreover, we model node features as multivariate random variables and introduce Feature Convolution Distance,
      FCDp, a Wasserstein distance-based pseudometric that matches the discriminative power of node-level MPNNs. We show that MPNNs are globally Lipschitz
      continuous functions with respect to FCDp. Our framework subsumes the deterministic case via Dirac measures and provides a foundation for reasoning about
      algorithmic stability in MPNNs with uncertainty in node features.
      </p>
    </div>
 
  </div>
      </div>
         <div class="projects-grid">
      <div class="project-card">
           
    <div class="project-content">
      <h4 class="project-title">Hyperbolic Geometry-Inspired Deep Neural Network Architectures</h4>

      <p class="project-text">
      Modern AI faces a critical efficiency crisis: DNNs require billions of parameters and consume enormous energy to achieve state-of-the-art performance, while remaining opaque ``black boxes'' whose decision-making processes cannot be understood or trusted. By grounding DNN architecture design in the geometric principles and hierarchical organization of natural neural networks, this project offers a principled path toward understanding how the resulting structural properties are related to learning efficiency, robustness, and generalization.

In our work, we introduce a novel DNN architecture that exploits the geometry of derandomized hyperbolic graphs. Concretely, we begin by generating a derandomized hyperbolic graph that encodes a hierarchical organization among neurons. We then transform this graph into a directed acyclic graph (DAG) that serves as the backbone of our neural architecture. Hidden variables assigned to each node in the graph impose a natural hierarchical order, which we use to direct edges and layer the network in a feed-forward manner. This construction ensures acyclicity and establishes an intrinsic layered structure dictated by the graph geometry. Each neuron (graph node) is assigned to a layer based on the longest path from the input nodes, thereby limiting extraneous connections and redundant message passing. The resulting architecture is a sparse, directed network that retains the expressivity of a dense network, but with far fewer parameters.

By reducing overlapping or unimportant paths, the architecture is not only more efficient but also more interpretable, since the hierarchical organization of connections aligns with clearer functional roles for each layer. This work represents a novel synthesis of ideas from statistical physics and deep learning, opening the door to principled new ways to design DNNs. 
      </p>
    </div>
      </div>
           <div class="project-card">
           
    <div class="project-content">
      <h4 class="project-title">FENCE</h4>

      <p class="project-text">
     text
      </p>
    </div>
      </div>
       <div class="project-card">
      <div class="project-content">
      <h4 class="project-title">SDC</h4>

      <p class="project-text">
    text
      </p>
    </div>
       </div>
        <div class="project-card">
      <div class="project-content">
      <h4 class="project-title">SPM</h4>

      <p class="project-text">
     text
      </p>
    </div>
       </div>
      </div>
       <div class="projects-grid">

<div class="project-card">
    <div class="project-content">
      <h4 class="project-title">SPM Defense</h4>
      

      <p class="project-links">
        <a href="" target="_blank"> ESORICS 2022</a>
      </p>

      <p class="project-text">
     text
      </p>
    </div>
 
  </div>
      </div>
  </div>

  <!-- PUBLICATIONS tab: move your existing Publications block here -->
  <div id="publications" class="tab-section" style="font-size: 17px">
    <ul>
      <li>
        <b>Robustness and Generalization in Uncertainty-aware Message Passing Neural Networks</b><br/>
        Alesia Chernikova, Moritz Laber, Narayan G. Sabhahit, Tina Eliassi-Rad</br>
        Under Submission
      </li>
          
      <li>
        <b>Uncertainty-aware Message Passing Neural Networks</b><br/>
        Alesia Chernikova, Moritz Laber, Narayan G. Sabhahit, Tina Eliassi-Rad<br/>
        <a href='https://openreview.net/pdf?id=bVToNxLO9W' target='_blank' style="color: #6b5b95; text-decoration: none">
           New Perspectives in Graph Machine Learning NeurIPS 2025
        </a>
      </li>
          
      <li>
            <b>Modeling Self-Propagating Malware with Epidemiological Models</b><br/>
           Alesia Chernikova, Nicolò Gozzi, Simona Boboila, Nicola Perra, Tina Eliassi-Rad, and Alina Oprea<br/>
           <a href = 'https://appliednetsci.springeropen.com/articles/10.1007/s41109-023-00578-z' target='_blank' style="color: #6b5b95; text-decoration: none">Applied Network Science (ANS) 2023</a>
      </li>
      <li>
           <b>Cyber Network Resilience against Self-Propagating Malware Attacks</b><br/>
           Alesia Chernikova, Nicolò Gozzi, Simona Boboila, Priyanka Angadi, John Loughner, Matthew Wilden, Nicola Perra, Tina Eliassi-Rad, and Alina Oprea<br/>
     <a href = 'https://link.springer.com/chapter/10.1007/978-3-031-17140-6_26' target='_blank' style="color: #6b5b95; text-decoration: none">European Symposium on Research in Computer Security (ESORICS) 2022 </a>
  </li>
   <li>
     <b>FENCE: Feasible Evasion Attacks on Neural Netwoks in Constrained Environments</b><br/>
   Alesia Chernikova and Alina Oprea<br/>
     <a href = 'https://doi.acm.org?doi=3544746' target='_blank' style="color: #6b5b95; text-decoration: none">ACM Transactions on Privacy and Security (ACM TOPS) 2022</a>
  </li>
  <li>
  <b>Are Self-Driving Cars Secure? Evasion Attacks against Deep Neural Networks for Steering Angle Prediction</b><br/>
  Alesia Chernikova, Alina Oprea, Cristina Nita-Rotaru and Baekgyu Kim<br/>
   <a href = 'https://arxiv.org/pdf/1904.07370.pdf' target='_blank' style="color: #6b5b95; text-decoration: none">IEEE Safe Things S&P 2019</a>
  </li>
  <li>
    <b>Hedging Algorithms Based on Interest-rate Swaps</b><br/>
    Alesia Chernikova and Vladimir Malugin<br/>
   In the 70th undergraduate, graduate, and postgraduate students scientific conference of Belarusian State University (vol. 1, pp. 242 – 245)
  </li>
    </ul>
  </div>
  <div id="teaching" class="tab-section" style="font-size: 17px">

<ul>
      <li>
            <b>CS 7332: Machine Learning with Graphs, Northeastern University, (Fall 2024, Fall 2025)</b><br/>
            <ul>
                  <li>
                        Designed and delivered graduate-level lectures on circuit tracing in Large Language Models 
                  </li>
                  <li>
                        Designed and delivered graduate-level lectures on cybernetwork resilience against adversarial attacks
                  </li>
            </ul>
      </li>
      <li>
           <b>CS 4100: Artificial Intelligence, Northeastern University, (Fall 2022, Fall 2023, Spring 2024)</b><br/>
            <ul>
                  <li>
                        Prepared and delivered lectures
                  </li>
                  <li>
                        Held weekly office hours
                  </li>
                  <li>
                        Graded assignments, exams, and research projects 
                  </li>
                  <li>
                        Assisted with homework and exam design, proctored exams
                  </li>
                  <li>
                        Advised students on research-oriented course projects
                  </li>
            </ul>      
      </li>
</ul>
  </div>
 <div id="talks" class="tab-section" style="font-size: 17px">
 <ul>
      <li>
            Uncertainty-Aware Message Passing Neural Networks. <font style="color: #ff7b25">NeurIPS, 2025: NPGML</font>
      </li>
      <li>
            Circuit Tracing in Large Language Models. <font style="color: #ff7b25">Network Science Institute, 2025</font>
      </li>
      <li>
            Modeling Self-propagating Malware with Compartmental Models of Epidemiology. <font style="color: #ff7b25">Joint Mathematics Meetings, 2025</font>
      </li>
       <li>
             Cybernetwork Resilience against Self-Propagating Malware Attacks. <font style="color: #ff7b25">Network Science Institute, 2024</font>
      </li>
       <li>
              Cybernetwork Resilience against Self-Propagating Malware Attacks.<font style="color: #ff7b25"> DoD SERDP Workshop, 2024</font>
       </li>
       <li>
            Towards Resilient Cybernetworks against Adversarial Attacks.<font style="color: #ff7b25"> Amazon Web Services, 2023</font>
       </li>
       <li>
             Cybernetwork Resilience against Self-Propagating Malware Attacks. <font style="color: #ff7b25"> ESORICS, 2022</font>
       </li>
      <li>
           Feasible Evasion Attacks in Constrained Environments.<font style="color: #ff7b25"> CRA Seminar, 2022</font>
      </li>
       <li>
             Graph-based Statistical Detection of Anomalous Role Assumption Events. <font style="color: #ff7b25">Amazon Web Services, 2020</font>
       </li>
       <li>
             Feasible Evasion Attacks on Neural Networks in Constrained Environments.<font style="color: #ff7b25">ARL Meeting, 2020</font>
       </li>
       <li>
             Evasion Attacks against Deep Neural Networks for Steering Angle Prediction. <font style="color: #ff7b25">IEEE S&P, 2019: SafeThings</font>
       </li>
 </ul>
 </div>
<div id="service" class="tab-section" style="font-size: 17px">
 <ul>
       <li>
             Program Committe: <font style="color: #ff7b25"> AISTATS'26, IEEE S&P'26, IEEE S&P'25, IEEE MILCOM AI for Cyber'23</font>
       </li>
       <li>
             Reviewer:  <font style="color: #ff7b25">ACM TOPS, IEEE Transactions on Privacy</font>
       </li>
 </ul>
</div>

  <footer>
    <a href="https://twitter.com/AlesiaChernikov" target='_blank' class="fa fa-twitter"></a>
    <a href="https://www.linkedin.com/in/alesia-chernikova" target='_blank' class="fa fa-linkedin"></a>
    <a href="https://github.com/achernikova" target='_blank' class="fa fa-github"></a>
  </footer>

<!-- NEW: tiny JS to switch tabs -->
<script>
  const buttons = document.querySelectorAll('.tab-link');
  const sections = document.querySelectorAll('.tab-section');

  buttons.forEach(btn => {
    btn.addEventListener('click', () => {
      // update buttons
      buttons.forEach(b => b.classList.remove('active'));
      btn.classList.add('active');

      // show correct section
      const targetId = btn.getAttribute('data-target');
      sections.forEach(sec => {
        if (sec.id === targetId) {
          sec.classList.add('active');
        } else {
          sec.classList.remove('active');
        }
      });
      localStorage.setItem('activeTab', targetId);
    });
  });
      
const saved = localStorage.getItem('activeTab');
  if (saved) {
    // find button + section for saved tab
    const btn = document.querySelector(`.tab-link[data-target="${saved}"]`);
    const sec = document.getElementById(saved);
    if (btn && sec) {
      buttons.forEach(b => b.classList.remove('active'));
      sections.forEach(s => s.classList.remove('active'));
      btn.classList.add('active');
      sec.classList.add('active');
    }
  }
      
</script>

</body>
</html>
